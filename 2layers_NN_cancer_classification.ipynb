{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data matrix: (3, 569)\n",
      "Shape of target vector: (1, 569)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([data.data[ : ,0],data.data[: ,1],data.data[: ,2]])\n",
    "X = X.reshape(3,569)\n",
    "y = data.target.reshape(1,569)\n",
    "print(\"Shape of data matrix: {}\".format(X.shape))\n",
    "print(\"Shape of target vector: {}\".format(y.shape))\n",
    "\n",
    "\n",
    "# Manuel split\n",
    "train_x = X[0:3, 0:398 ]\n",
    "train_y = y[0:1, 0: 398]\n",
    "test_x =  X[0:3, 398: ]\n",
    "test_y =  y[0:1, 398: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 398\n",
      "Number of testing examples: 171\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset \n",
    "m_train = train_x.shape[1]\n",
    "m_test = test_x.shape[1]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z \n",
    "    return A,cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "def initialize_params_deep(layer_dims):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['w' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['w' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 3     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache =  linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache =  linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache =  linear_activation_forward(A, parameters['W' + str(L)] ,parameters['b' + str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost =  - np.sum(np.multiply(np.log(AL), Y) + np.multiply((1 - Y), np.log(1 - AL)))  / m\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, cache[1])\n",
    "        dA_prev, dW, db =  linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, cache[1])\n",
    "        dA_prev, dW, db =  linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_layer_model\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "\n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "\n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6930140885301452\n",
      "Cost after iteration 100: 0.6859598081586793\n",
      "Cost after iteration 200: 0.6783369492029321\n",
      "Cost after iteration 300: 0.6689188217584288\n",
      "Cost after iteration 400: 0.6558850125924918\n",
      "Cost after iteration 500: 0.6405849418475765\n",
      "Cost after iteration 600: 0.6273160117199147\n",
      "Cost after iteration 700: 0.6174846219905369\n",
      "Cost after iteration 800: 0.6109857396893058\n",
      "Cost after iteration 900: 0.6046516082081552\n",
      "Cost after iteration 1000: 0.6001166021773251\n",
      "Cost after iteration 1100: 0.59515243910004\n",
      "Cost after iteration 1200: 0.5910902037786392\n",
      "Cost after iteration 1300: 0.5884959717104679\n",
      "Cost after iteration 1400: 0.5794684863332049\n",
      "Cost after iteration 1500: 0.5756643492227749\n",
      "Cost after iteration 1600: 0.5726639387003256\n",
      "Cost after iteration 1700: 0.5708807478647017\n",
      "Cost after iteration 1800: 0.5683168831899216\n",
      "Cost after iteration 1900: 0.5669531159921055\n",
      "Cost after iteration 2000: 0.5602952220617786\n",
      "Cost after iteration 2100: 0.559129803760198\n",
      "Cost after iteration 2200: 0.5547258165300418\n",
      "Cost after iteration 2300: 0.5506106807070846\n",
      "Cost after iteration 2400: 0.5444986026648934\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8187134502923976\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(test_x, test_y, parameters)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
